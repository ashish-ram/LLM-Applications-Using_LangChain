{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics - 1\n",
    "\n",
    "Basic connectivity with the GCP Vertex AI using python sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "# from google.oauth2 import service_account\n",
    "\n",
    "# credentials = service_account.Credentials.from_service_account_file(\n",
    "#     'one-ai-a826dd60ad96.json')\n",
    "# vertexai.init(project=\"one-ai\", location=\"us-central1\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"one-ai\", location=\"us-central1\")\n",
    "parameters = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_output_tokens\": 256,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: India is a vast and diverse country with a rich history and culture. There are many different places to visit in India, each with its own unique charm. Some of the most popular tourist destinations include:\n",
      "\n",
      "* **Taj Mahal**, Agra: The Taj Mahal is one of the most iconic landmarks in the world. It is a beautiful white marble mausoleum built by Mughal emperor Shah Jahan in memory of his wife, Mumtaz Mahal.\n",
      "* **Golden Temple**, Amritsar: The Golden Temple is a Sikh holy site located in the city of Amritsar. It is a beautiful temple complex that is open to people of all faiths.\n",
      "* **Khajuraho**, Madhya Pradesh: Khajuraho is a UNESCO World Heritage Site known for its many Hindu and Jain temples that are decorated with erotic sculptures.\n",
      "* **Goa:** Goa is a popular beach destination located on the west coast of India. It is known for its beautiful beaches, lively nightlife, and Portuguese colonial architecture.\n",
      "* **Varanasi:** Varanasi is a holy city located on the banks of the Ganges River. It is a popular destination for pilgrims and tourists alike. The city is known for its ghats, where people come to bathe in the Ganges River\n"
     ]
    }
   ],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(\n",
    "    \"\"\"what are the best places to visit in india?\"\"\",\n",
    "    **parameters\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics - 2\n",
    "\n",
    "Using LangChain Chain with Foundation LLM and Chatbased LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '_type': 'prompt',\n",
      "    'input_variables': ['question'],\n",
      "    'output_parser': None,\n",
      "    'partial_variables': {},\n",
      "    'template': \"Question: {question}\\n\\nAnswer: Let's think step by step.\",\n",
      "    'template_format': 'f-string',\n",
      "    'validate_template': True}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# pp.pprint(json.dumps(prompt.dict(), indent=4))\n",
    "pp.pprint(prompt.dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Justin Beiber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\\nThe final answer: San Francisco 49ers.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now leverage the Codey API for code generation within Vertex AI. The model names are:\n",
    "\n",
    "# code-bison: for code suggestion\n",
    "# code-gecko: for code completion\n",
    "\n",
    "llm = VertexAI(model_name=\"code-bison\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\ndef is_prime(n):\\n  \"\"\"\\n  Determines if the given number is prime.\\n\\n  Args:\\n    n: The number to be tested.\\n\\n  Returns:\\n    True if the number is prime, False otherwise.\\n  \"\"\"\\n\\n  # Check if the number is 1.\\n  if n == 1:\\n    return False\\n\\n  # Check if the number is 2.\\n  if n == 2:\\n    return True\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Write a python function that identifies if the number is a prime number?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_prime(n):\n",
      "  \"\"\"\n",
      "  Determines if the given number is prime.\n",
      "\n",
      "  Args:\n",
      "    n: The number to be tested.\n",
      "\n",
      "  Returns:\n",
      "    True if the number is prime, False otherwise.\n",
      "  \"\"\"\n",
      "\n",
      "  # Check if the number is 1.\n",
      "  if n == 1:\n",
      "    return False\n",
      "\n",
      "  # Check if the number is 2.\n",
      "  if n == 2:\n",
      "    return True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='India is a vast and diverse country with a rich history and culture. There are many different places to visit in India, each with its own unique charm. Here are some of the top places to visit in India:\\n\\n* **Taj Mahal**\\n\\nThe Taj Mahal is one of the most iconic landmarks in the world. It is a beautiful white marble mausoleum built by Mughal emperor Shah Jahan in memory of his wife, Mumtaz Mahal. The Taj Mahal is located in Agra, Uttar Pradesh.\\n\\n* **Golden Temple**\\n\\nThe Golden Temple is a Sikh temple located in Amritsar, Punjab. It is', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the chatbased PaLM Chat based Model in Vertex AI\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = VertexAI(temperature=0)\n",
    "chat.predict_messages([HumanMessage(content=\"Tell me about top places to visit in India\")])\n",
    "# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling PaLM LLM\n",
    "\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatVertexAI(temperature=0)\n",
    "chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n",
    "# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes shampoo and soap'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product1} and {product2}\")\n",
    "prompt.format(product1 = \"shampoo\", product2=\"soap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In ChatModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    " - LLM + Prompt\n",
    " - Is a way to pipe templatized prompts to LLMs/ChatModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chocolaty Creamy Delights'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "llm = VertexAI()\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product1} and {product2}\")\n",
    "prompt.format(product1 = \"shampoo\", product2=\"soap\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run({'product1': \"chocolates\", \"product2\":\"icecreams\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In ChatModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts.chat import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "\n",
    "chat_model = VertexAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistance that translate {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"German\", text=\"I love programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! I'm a big fan of programming too. What languages do you like to work with?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatVertexAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant. Have a friendly conversation with the human companion. Engage with the human by asking followup questions and sharing your experience and\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm glad you're enjoying it. I find it to be a good way to exercise my mind as well.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain.run(\"It is a good mental challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "**Agent = LLM + Chain + Tools + Prompt**\n",
    "\n",
    "__Steps__:\n",
    "\n",
    "    - Initialize LLM model or chatmodel\n",
    "    - load tools \n",
    "    - initialize an Agent with Agent type, LLM, loaded tools \n",
    "    - pass prompts to Agent\n",
    "\n",
    "**Agents still don't have any memory, so they can not engage in any conversation as they won't any context to answer followup questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up API key for SERP (search engine tool)\n",
    "# !export SERPAPI_API_KEY=\"b558b707546ad2ee5a781efd0092321c882b5f88daba246aa254eb752ab4f83a\"\n",
    "\n",
    "import os\n",
    "os.environ['SERPAPI_API_KEY']='b558b707546ad2ee5a781efd0092321c882b5f88daba246aa254eb752ab4f83a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to know the high temperature in SF yesterday in Fahrenheit\n",
      "Action: Search\n",
      "Action Input: \"high temperature in SF yesterday in Fahrenheit\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSan Francisco Temperature Yesterday. Maximum temperature yesterday: 75 째F (at 1:56 pm) Minimum temperature yesterday: 55 째F (at 5:56 am) Average temperature ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI know the high temperature in SF yesterday was 75 degrees Fahrenheit\n",
      "Action: Calculator\n",
      "Action Input: \"75^.023\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1044000282035853\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 1.1044000282035853\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1044000282035853'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seding a task to the LLM model in REACT framework which forces the model to answer the \n",
    "# question in steps, giving it time to think and verifying each step\n",
    "# this is popular Prompt Engineering method \n",
    "\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm = VertexAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1044000282035853"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.pow(75,0.023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ChatModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obersations on VertexAI's PaLM Chatmodel\n",
    "\n",
    "PaLMs chatmodels have worse compared to the LLM version models in following instruction to use external tools. Instead because these models are trained to follow instuctions, they \n",
    "tend to use the model itself to generate response while the best decision at that point was to use the external tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? Calculate the value of his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's boyfriend is.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde's boyfriend\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWilde began dating American actor and comedian Jason Sudeikis in November 2011. They became engaged in January 2013. The couple have two children: a son, born in 2014, and a daughter, born in 2016.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI found that Olivia Wilde's boyfriend is Jason Sudeikis.\n",
      "Question: Who is Olivia Wilde's boyfriend? Calculate the value of his current age raised to the 0.23 power?\n",
      "Thought: I need to find out Jason Sudeikis's current age.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Jason Sudeikis's age\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m47 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI found that Jason Sudeikis is 47 years old.\n",
      "Question: Who is Olivia Wilde's boyfriend? Calculate the value of his current age raised to the 0.23 power?\n",
      "Thought: I need to calculate the value of Jason Sudeikis's current age raised to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"47^0.23\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.4242784855673896\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: 2.4242784855673896\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4242784855673896'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatVertexAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = VertexAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? Calculate the value of his current age raised to the 0.23 power?\")\n",
    "\n",
    "\n",
    "###############\n",
    "## Different Text input and their results\n",
    "# Who is Olivia Wilde's boyfriend? Calculate the value of his current age raised to the 0.23 power? --> correct answer = 2.4\n",
    "# Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power? --> Wrong answer bcoz it does not use the calculator and just generate some text garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.412316577267772"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.pow(46,0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\n",
      "Thought: I need to find the high temperature in SF yesterday in Fahrenheit.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"high temperature in SF yesterday\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSan Francisco Temperature Yesterday. Maximum temperature yesterday: 75 째F (at 1:56 pm) Minimum temperature yesterday: 55 째F (at 5:56 am) Average temperature ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe high temperature in SF yesterday was 75 degrees Fahrenheit.\n",
      "Final Answer: 75.023\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'75.023'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatVertexAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = VertexAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi! How are you doing today?'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain import ConversationChain\n",
    "\n",
    "llm = VertexAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "conversation.run(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hi! How are you doing today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! I'm always happy to chat.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
