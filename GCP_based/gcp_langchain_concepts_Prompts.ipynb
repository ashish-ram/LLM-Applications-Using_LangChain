{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain template class\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Generate a name for the company that produces {product}\")\n",
    "prompt.format(product = \"shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example prompt with no input variables\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
    "no_input_prompt.format()\n",
    "# -> \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example prompt with one input variable\n",
    "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
    "one_input_prompt.format(adjective=\"funny\")\n",
    "# -> \"Tell me a funny joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example prompt with multiple input variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"], \n",
    "    template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\n",
    "# -> \"Tell me a funny joke about chickens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Tell me a {adjective} joke about {content}.\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.input_variables\n",
    "# -> ['adjective', 'content']\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "# -> Tell me a funny joke about chickens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts for Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "assert system_message_prompt == system_message_prompt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Feature Store - Using Feast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "- create a prompt template of template. The idea is that, this will be completed by querying the feature store. The output is a prompt template.\n",
    "- now the process is same as before. Create a model/llm object, create a chain object with that llm and prompt and then run the chain.\n",
    "- this process is exactly same for any other feature stores like Teckon ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from feast import FeatureStore\n",
    "from feast.data_source import PushMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_historical_features_entity_df(store: FeatureStore, for_batch_scoring: bool):\n",
    "    # Note: see https://docs.feast.dev/getting-started/concepts/feature-retrieval for more details on how to retrieve\n",
    "    # for all entities in the offline store instead\n",
    "    entity_df = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            # entity's join key -> entity values\n",
    "            \"driver_id\": [1001, 1002, 1003],\n",
    "            # \"event_timestamp\" (reserved key) -> timestamps\n",
    "            \"event_timestamp\": [\n",
    "                datetime(2021, 4, 12, 10, 59, 42),\n",
    "                datetime(2021, 4, 12, 8, 12, 10),\n",
    "                datetime(2021, 4, 12, 16, 40, 26),\n",
    "            ],\n",
    "            # (optional) label name -> label values. Feast does not process these\n",
    "            \"label_driver_reported_satisfaction\": [1, 5, 3],\n",
    "            # values we're using for an on-demand transformation\n",
    "            \"val_to_add\": [1, 2, 3],\n",
    "            \"val_to_add_2\": [10, 20, 30],\n",
    "        }\n",
    "    )\n",
    "    # For batch scoring, we want the latest timestamps\n",
    "    if for_batch_scoring:\n",
    "        entity_df[\"event_timestamp\"] = pd.to_datetime(\"now\", utc=True)\n",
    "\n",
    "    training_df = store.get_historical_features(\n",
    "        entity_df=entity_df,\n",
    "        features=[\n",
    "            \"driver_hourly_stats:conv_rate\",\n",
    "            \"driver_hourly_stats:acc_rate\",\n",
    "            \"driver_hourly_stats:avg_daily_trips\",\n",
    "            \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "            \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "        ],\n",
    "    ).to_df()\n",
    "    print(training_df.head())\n",
    "\n",
    "\n",
    "def fetch_online_features(store, source: str = \"\"):\n",
    "    entity_rows = [\n",
    "        # {join_key: entity_value}\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "            \"val_to_add\": 1000,\n",
    "            \"val_to_add_2\": 2000,\n",
    "        },\n",
    "        {\n",
    "            \"driver_id\": 1002,\n",
    "            \"val_to_add\": 1001,\n",
    "            \"val_to_add_2\": 2002,\n",
    "        },\n",
    "    ]\n",
    "    if source == \"feature_service\":\n",
    "        features_to_fetch = store.get_feature_service(\"driver_activity_v1\")\n",
    "    elif source == \"push\":\n",
    "        features_to_fetch = store.get_feature_service(\"driver_activity_v3\")\n",
    "    else:\n",
    "        features_to_fetch = [\n",
    "            \"driver_hourly_stats:acc_rate\",\n",
    "            \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "            \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "        ]\n",
    "    returned_features = store.get_online_features(\n",
    "        features=features_to_fetch,\n",
    "        entity_rows=entity_rows,\n",
    "    ).to_dict()\n",
    "    for key, value in sorted(returned_features.items()):\n",
    "        print(key, \" : \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FeatureStore(repo_path=\"./../my_project/feature_repo/\")\n",
    "print(\"\\n--- Run feast apply ---\")\n",
    "subprocess.run([\"feast\", \"apply\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "# You may need to update the path depending on where you stored it\n",
    "feast_repo_path = \"./../my_project/feature_repo/\"\n",
    "store = FeatureStore(repo_path=feast_repo_path)\n",
    "\n",
    "# import subprocess\n",
    "# subprocess.run([\"feast\", \"apply\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Historical features for training ---\")\n",
    "fetch_historical_features_entity_df(store, for_batch_scoring=False)\n",
    "\n",
    "print(\"\\n--- Historical features for batch scoring ---\")\n",
    "fetch_historical_features_entity_df(store, for_batch_scoring=True)\n",
    "\n",
    "print(\"\\n--- Load features into online store ---\")\n",
    "store.materialize_incremental(end_date=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Online features ---\")\n",
    "fetch_online_features(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Online features retrieved (instead) through a feature service---\")\n",
    "fetch_online_features(store, source=\"feature_service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, StringPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the driver's up to date stats, write them note relaying those stats to them.\n",
    "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
    "\n",
    "Here are the drivers stats:\n",
    "Conversation rate: {conv_rate}\n",
    "Acceptance rate: {acc_rate}\n",
    "Average Daily Trips: {avg_daily_trips}\n",
    "\n",
    "Your response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeastPromptTemplate(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        driver_id = kwargs.pop(\"driver_id\")\n",
    "        feature_vector = store.get_online_features(\n",
    "            features=[\n",
    "                \"driver_hourly_stats:conv_rate\",\n",
    "                \"driver_hourly_stats:acc_rate\",\n",
    "                \"driver_hourly_stats:avg_daily_trips\",\n",
    "            ],\n",
    "            entity_rows=[{\"driver_id\": driver_id}],\n",
    "        ).to_dict()\n",
    "        kwargs[\"conv_rate\"] = feature_vector[\"conv_rate\"][0]\n",
    "        kwargs[\"acc_rate\"] = feature_vector[\"acc_rate\"][0]\n",
    "        kwargs[\"avg_daily_trips\"] = feature_vector[\"avg_daily_trips\"][0]\n",
    "        return prompt.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FeastPromptTemplate(input_variables=[\"driver_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.get_online_features(\n",
    "        features=[\n",
    "            \"driver_hourly_stats:conv_rate\",\n",
    "            \"driver_hourly_stats:acc_rate\",\n",
    "            \"driver_hourly_stats:avg_daily_trips\",\n",
    "        ],\n",
    "        entity_rows=[{\"driver_id\": '1005'}],\n",
    "    ).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.format(driver_id=1005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=ChatVertexAI(temperature=0.5), prompt=prompt_template)\n",
    "\n",
    "chain.run(1005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kind of custom prompt templates\n",
    "- String prompt templates\n",
    "- chat prompt templates\n",
    "\n",
    "To create a custom string prompt template, there are two requirements:\n",
    "\n",
    "- It has an input_variables attribute that exposes what input variables the prompt template expects.\n",
    "- It exposes a format method that takes in keyword arguments corresponding to the expected input_variables and returns the formatted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate):\n",
    "    \"\"\" a custom prompt template that takes in the function name as input, and formats the prompt template\n",
    "    to provide the source code of the function\"\"\"\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variable(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable\")\n",
    "        return v\n",
    "    def format(self, **kwargs)->str:\n",
    "        # get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the llm\n",
    "        prompt = f\"\"\"\n",
    "            Given the function name and the source code, generate an English language explaination of the function.\n",
    "            Function Name: {kwargs['function_name'].__name__}\n",
    "            Source Code : \n",
    "            {source_code}\n",
    "            Explaination: \n",
    "            \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.format(driver_id=1001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain1 = LLMChain(llm=ChatVertexAI(temperature=0.7), prompt=fn_explainer)\n",
    "\n",
    "chain1.run(function_name=fetch_online_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are prompts with exmples included in the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of useful methods:\n",
    "\n",
    "- LLMs are callable. So you can directly call them like llm(\"tell me a joke\")\n",
    "\n",
    "- Or you can use the generate() method. generate lets you can call the model with a list of strings, getting back a more complete response than just the text. This complete response can includes things like multiple top responses and other LLM provider-specific information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.vertexai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain.llms.vertexai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain.llms.vertexai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain.llms.vertexai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "Because he was outstanding in his field!\n",
      "generations=[[Generation(text='Why did the scarecrow win an award? Because he was outstanding in his field!', generation_info=None)], [Generation(text='Why did the scarecrow win an award?\\nBecause he was outstanding in his field!', generation_info=None)]] llm_output=None run=[RunInfo(run_id=UUID('bf6ea96a-4438-44b6-896b-60d613266e9f')), RunInfo(run_id=UUID('f149b5da-1df7-4f2b-b6f1-5d76893d2268'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "llm = VertexAI(temperature=0.7)\n",
    "\n",
    "print(llm(\"Tell me a joke\"))\n",
    "\n",
    "print(llm.generate([\"Tell me a joke\"]*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
